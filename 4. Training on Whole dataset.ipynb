{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "6NorJgDixWJg",
    "outputId": "ca4ddbed-6a49-4744-f1c2-33c55af543f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "#https://drive.google.com/open?id=1NVfn_L9YoGJloB_rzKh06YCqQNe6DfJj\n",
    "%cd /content\n",
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90o9HSgfxcKQ"
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/open?id=1RrYL4G-meAFho_pgfoL_drxhSn5hY1qI\n",
    "downloaded = drive.CreateFile({'id': '1RrYL4G-meAFho_pgfoL_drxhSn5hY1qI'})\n",
    "downloaded.GetContentFile('full_answers_train.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=1NOP8_ezkheF0jw6k4CrDqqah_wPmv10E\n",
    "downloaded = drive.CreateFile({'id': '1NOP8_ezkheF0jw6k4CrDqqah_wPmv10E'})\n",
    "downloaded.GetContentFile('full_answers_val.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=1SZSnc2LnQ_NRuXzXkYDQS0OXGNZDvpTw\n",
    "downloaded = drive.CreateFile({'id': '1SZSnc2LnQ_NRuXzXkYDQS0OXGNZDvpTw'})\n",
    "downloaded.GetContentFile('full_img_features_train.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=1G8S939v1iNbCuhREOlcLIB5EdrOuWgC8\n",
    "downloaded = drive.CreateFile({'id': '1G8S939v1iNbCuhREOlcLIB5EdrOuWgC8'})\n",
    "downloaded.GetContentFile('full_img_features_val.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=1b0qu55cmFP_V213N26RsW19dtjrZo104\n",
    "downloaded = drive.CreateFile({'id': '1b0qu55cmFP_V213N26RsW19dtjrZo104'})\n",
    "downloaded.GetContentFile('full_question_train_tokenize.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=105lIe-mBRlXNtiE-ribjO1Ft0pMY8pYE\n",
    "downloaded = drive.CreateFile({'id': '105lIe-mBRlXNtiE-ribjO1Ft0pMY8pYE'})\n",
    "downloaded.GetContentFile('full_question_val_tokenize.h5')\n",
    "\n",
    "# https://drive.google.com/open?id=1zQvl9cYhqk2bRsXWMrnNUn_tvab3XRQS\n",
    "downloaded = drive.CreateFile({'id': '1zQvl9cYhqk2bRsXWMrnNUn_tvab3XRQS'})\n",
    "downloaded.GetContentFile('embedding_matrix_tokenize.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE3p8Wk5nsc6"
   },
   "outputs": [],
   "source": [
    "# https://drive.google.com/open?id=1qmDX_URx9rKqjBZczX0hINk5uA4kI9SA\n",
    "downloaded = drive.CreateFile({'id': '1qmDX_URx9rKqjBZczX0hINk5uA4kI9SA'})\n",
    "downloaded.GetContentFile('Training Data QA.pickle')\n",
    "\n",
    "# https://drive.google.com/open?id=1vBVqBh-caYYat7ZXBleJWHGV0al1I4nJ\n",
    "downloaded = drive.CreateFile({'id': '1vBVqBh-caYYat7ZXBleJWHGV0al1I4nJ'})\n",
    "downloaded.GetContentFile('Validation Data QA.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayE0gst7xlYU"
   },
   "outputs": [],
   "source": [
    "# For full vqa training\n",
    "import h5py\n",
    "import numpy as np\n",
    "h5_img = h5py.File('full_img_features_train.h5', 'r')\n",
    "img_features_train = h5_img['full_img_features_train'][:]\n",
    "\n",
    "h5_ans = h5py.File('full_answers_train.h5', 'r')\n",
    "answer_train = h5_ans['full_answers_train'][:]\n",
    "h5_ans.close()\n",
    "\n",
    "h5_que = h5py.File('full_question_train_tokenize.h5', 'r')\n",
    "question_train = h5_que['full_question_train_tokenize'][:]\n",
    "\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1EvIl0QlgKW"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "h5_img = h5py.File('full_img_features_train.h5', 'r')\n",
    "img_features_train = h5_img['full_img_features_train'][:]\n",
    "image_train = np.repeat(img_features_train, 3, 0)\n",
    "\n",
    "h5f = h5py.File('image_train.h5', 'w')\n",
    "h5f.create_dataset('image_train', data=image_train)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA9ZznAN_Ihi"
   },
   "outputs": [],
   "source": [
    "# Terminate the session and reconnect to clear the RAM, and then run this cell.\n",
    "import h5py\n",
    "import numpy as np\n",
    "h5_img = h5py.File('full_img_features_val.h5', 'r')\n",
    "img_features_train = h5_img['full_img_features_val'][:]\n",
    "image_val = np.repeat(img_features_train, 3, 0)\n",
    "\n",
    "h5f = h5py.File('image_val.h5', 'w')\n",
    "h5f.create_dataset('image_val', data=image_val)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4vcRbjLz2hq0",
    "outputId": "7e1496cc-ad79-4012-ae90-4e85bf322d31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.io_utils import HDF5Matrix\n",
    "\n",
    "def generator(answer, img, question, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_img =      HDF5Matrix(img, 'image_train')\n",
    "        x_question = HDF5Matrix(question, 'full_question_train_tokenize')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_question.end\n",
    "        #X_img = np.repeat(x_img, 3, 0)\n",
    "    elif types == 'test':\n",
    "        x_img =      HDF5Matrix(img, 'image_val')\n",
    "        x_question = HDF5Matrix(question, 'full_question_val_tokenize')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_question.end\n",
    "        #X_img = np.repeat(x_img, 3, 0)\n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield [x_img[idx:end], x_question[idx:end]], y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJPMg5Zo47tz"
   },
   "source": [
    "# 1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "colab_type": "code",
    "id": "sUHv9RIH1cbn",
    "outputId": "a8badc09-ffa6-458e-e2b0-11264f681d61"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 15:14:04.893018 140598088169344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 15:14:06.678707 140598088169344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          (None, 512)          1667072     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           cu_dnnlstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         525312      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1001)         2051049     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1001)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         1003002     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,274,763\n",
      "Trainable params: 9,441,763\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os, pandas as pd, numpy as np\n",
    "import tensorflow.keras.backend as k\n",
    "import h5py, pickle\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, CuDNNLSTM, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "k.clear_session()\n",
    "\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "\n",
    "NAME = \"1_model_one_lstm_vgg\"\n",
    "\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "loss = []\n",
    "acc = []\n",
    "\n",
    "dropout_rate = 0.5\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "#x = Embedding(output_dim=512, input_dim=125, input_length=25)(question_input)\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "lstm_1 = CuDNNLSTM(units=512, return_sequences=False)(x)\n",
    "dropout__ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "#lstm_2 = CuDNNLSTM(units=512, return_sequences=False)(dropout__ques_1)\n",
    "#dropout__ques_2 = Dropout(dropout_rate)(lstm_2)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout__ques_1)\n",
    "print (\"Creating image model...\")\n",
    "\n",
    "image_input = Input(shape=(4096, ) )\n",
    "#reshape = reshape((4096,))(image_input)\n",
    "dense_img_1 = Dense(1024,  activation='relu')(image_input)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_1, dense_ques_1])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_1 = Model(inputs=[image_input, question_input], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model_1.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xDu_8wnLQW8"
   },
   "source": [
    "We will train this model through:<Br/>\n",
    "1) Keras Train_on_batch function<Br/>\n",
    "2) Python Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Yy1JUaDLmq4"
   },
   "source": [
    "## Train_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kCfQg4F4qmR",
    "outputId": "c4ab5da6-cdee-4255-bd58-bef3f1e60ebc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the data\n",
      "epoch started!\n",
      "Epoch 1/50\n",
      "cost: 2.222215, acc: 0.341463\n",
      "cost average: 2.754924, acc average: 0.247960\n",
      "Epoch 2/50\n",
      "cost: 1.897433, acc: 0.373984\n",
      "cost average: 1.993711, acc average: 0.356104\n",
      "Epoch 3/50\n",
      "cost: 1.776995, acc: 0.439024\n",
      "cost average: 1.853145, acc average: 0.372557\n",
      "Epoch 4/50\n",
      "cost: 1.812336, acc: 0.447154\n",
      "cost average: 1.806862, acc average: 0.380256\n",
      "Epoch 5/50\n",
      "cost: 1.770346, acc: 0.455285\n",
      "cost average: 1.777000, acc average: 0.387580\n",
      "Epoch 6/50\n",
      "cost: 1.778272, acc: 0.422764\n",
      "cost average: 1.751572, acc average: 0.394302\n",
      "Epoch 7/50\n",
      "cost: 1.770441, acc: 0.398374\n",
      "cost average: 1.728532, acc average: 0.401198\n",
      "Epoch 8/50\n",
      "cost: 1.739548, acc: 0.422764\n",
      "cost average: 1.708645, acc average: 0.407890\n",
      "Epoch 9/50\n",
      "cost: 1.726582, acc: 0.422764\n",
      "cost average: 1.688667, acc average: 0.413336\n",
      "Epoch 10/50\n",
      "cost: 1.641065, acc: 0.447154\n",
      "cost average: 1.669165, acc average: 0.420668\n",
      "Epoch 11/50\n",
      "cost: 1.654314, acc: 0.463415\n",
      "cost average: 1.649562, acc average: 0.427785\n",
      "Epoch 12/50\n",
      "cost: 1.640489, acc: 0.463415\n",
      "cost average: 1.633881, acc average: 0.434112\n",
      "Epoch 13/50\n",
      "cost: 1.686236, acc: 0.439024\n",
      "cost average: 1.613730, acc average: 0.439293\n",
      "Epoch 14/50\n",
      "cost: 1.630154, acc: 0.447154\n",
      "cost average: 1.594946, acc average: 0.446150\n",
      "Epoch 15/50\n",
      "cost: 1.601940, acc: 0.439024\n",
      "cost average: 1.575416, acc average: 0.451565\n",
      "Epoch 16/50\n",
      "cost: 1.621212, acc: 0.471545\n",
      "cost average: 1.555954, acc average: 0.458130\n",
      "Epoch 17/50\n",
      "cost: 1.585012, acc: 0.487805\n",
      "cost average: 1.536535, acc average: 0.465827\n",
      "Epoch 18/50\n",
      "cost: 1.533203, acc: 0.479675\n",
      "cost average: 1.517632, acc average: 0.470968\n",
      "Epoch 19/50\n",
      "cost: 1.614835, acc: 0.471545\n",
      "cost average: 1.499822, acc average: 0.477293\n",
      "Epoch 20/50\n",
      "cost: 1.572460, acc: 0.430894\n",
      "cost average: 1.478497, acc average: 0.484315\n",
      "Epoch 21/50\n",
      "cost: 1.476392, acc: 0.520325\n",
      "cost average: 1.459332, acc average: 0.490726\n",
      "Epoch 22/50\n",
      "cost: 1.610703, acc: 0.520325\n",
      "cost average: 1.439457, acc average: 0.496344\n",
      "Epoch 23/50\n",
      "cost: 1.516450, acc: 0.495935\n",
      "cost average: 1.418976, acc average: 0.502738\n",
      "Epoch 24/50\n",
      "cost: 1.523474, acc: 0.430894\n",
      "cost average: 1.398765, acc average: 0.509712\n",
      "Epoch 25/50\n",
      "cost: 1.515488, acc: 0.504065\n",
      "cost average: 1.376078, acc average: 0.516546\n",
      "Epoch 26/50\n",
      "cost: 1.525903, acc: 0.471545\n",
      "cost average: 1.356215, acc average: 0.522530\n",
      "Epoch 27/50\n",
      "cost: 1.372636, acc: 0.536585\n",
      "cost average: 1.336362, acc average: 0.530506\n",
      "Epoch 28/50\n",
      "cost: 1.478908, acc: 0.487805\n",
      "cost average: 1.315717, acc average: 0.537219\n",
      "Epoch 29/50\n",
      "cost: 1.427998, acc: 0.504065\n",
      "cost average: 1.296709, acc average: 0.542596\n",
      "Epoch 30/50\n",
      "cost: 1.426559, acc: 0.504065\n",
      "cost average: 1.276094, acc average: 0.549523\n",
      "Epoch 31/50\n",
      "cost: 1.428118, acc: 0.536585\n",
      "cost average: 1.256946, acc average: 0.555759\n",
      "Epoch 32/50\n",
      "cost: 1.316470, acc: 0.536585\n",
      "cost average: 1.235615, acc average: 0.563141\n",
      "Epoch 33/50\n",
      "cost: 1.440994, acc: 0.479675\n",
      "cost average: 1.217762, acc average: 0.567979\n",
      "Epoch 34/50\n",
      "cost: 1.469849, acc: 0.495935\n",
      "cost average: 1.200325, acc average: 0.574061\n",
      "Epoch 35/50\n",
      "cost: 1.387907, acc: 0.479675\n",
      "cost average: 1.184264, acc average: 0.579933\n",
      "Epoch 36/50\n",
      "cost: 1.403684, acc: 0.504065\n",
      "cost average: 1.163291, acc average: 0.586133\n",
      "Epoch 37/50\n",
      "cost: 1.417376, acc: 0.552846\n",
      "cost average: 1.149996, acc average: 0.591616\n",
      "Epoch 38/50\n",
      "cost: 1.486684, acc: 0.528455\n",
      "cost average: 1.128848, acc average: 0.598335\n",
      "Epoch 39/50\n",
      "cost: 1.397624, acc: 0.520325\n",
      "cost average: 1.113753, acc average: 0.602985\n",
      "Epoch 40/50\n",
      "cost: 1.359082, acc: 0.536585\n",
      "cost average: 1.100698, acc average: 0.606767\n",
      "Epoch 41/50\n",
      "cost: 1.280761, acc: 0.585366\n",
      "cost average: 1.081728, acc average: 0.612769\n",
      "Epoch 42/50\n",
      "cost: 1.304923, acc: 0.528455\n",
      "cost average: 1.066268, acc average: 0.619250\n",
      "Epoch 43/50\n",
      "cost: 1.368743, acc: 0.536585\n",
      "cost average: 1.053800, acc average: 0.622901\n",
      "Epoch 44/50\n",
      "cost: 1.351645, acc: 0.585366\n",
      "cost average: 1.038691, acc average: 0.627413\n",
      "Epoch 45/50\n",
      "cost: 1.283505, acc: 0.569106\n",
      "cost average: 1.025373, acc average: 0.632105\n",
      "Epoch 46/50\n",
      "cost: 1.320791, acc: 0.560976\n",
      "cost average: 1.013410, acc average: 0.636408\n",
      "Epoch 47/50\n",
      "cost: 1.236132, acc: 0.601626\n",
      "cost average: 0.998791, acc average: 0.641542\n",
      "Epoch 48/50\n",
      "cost: 1.307010, acc: 0.585366\n",
      "cost average: 0.985671, acc average: 0.646064\n",
      "Epoch 49/50\n",
      "cost: 1.390162, acc: 0.495935\n",
      "cost average: 0.975703, acc average: 0.649106\n",
      "Epoch 50/50\n",
      "cost: 1.252344, acc: 0.593496\n",
      "cost average: 0.961140, acc average: 0.654456\n"
     ]
    }
   ],
   "source": [
    "# using keras tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "nb_epoch = 50\n",
    "batch_size = 66  #99\n",
    "\n",
    "h5_img = h5py.File('full_img_features_train.h5', 'r')\n",
    "img_features_train = h5_img['full_img_features_train'][:]\n",
    "\n",
    "h5_ans = h5py.File('full_answers_train.h5', 'r')\n",
    "answer_train = h5_ans['full_answers_train'][:]\n",
    "\n",
    "h5_que = h5py.File('full_question_train_tokenize.h5', 'r')\n",
    "question_train = h5_que['full_question_train_tokenize'][:]\n",
    "\n",
    "print(\"read the data\")\n",
    "\n",
    "print(\"epoch started!\")\n",
    "for e in range(nb_epoch):\n",
    "    epoch_loss = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    print(\"Epoch {}/{}\".format((e+1), nb_epoch))\n",
    "    nb_batch = 248349//batch_size\n",
    "      \n",
    "    for idy in range(nb_batch):\n",
    "        start = idy * batch_size\n",
    "        if idy == nb_batch - 1:\n",
    "            end = 248349\n",
    "        else:\n",
    "            end = start + batch_size\n",
    "  \n",
    "    start_feat = start // 3\n",
    "    end_feat = end // 3\n",
    "    X_img = img_features_train[start_feat:end_feat,:]\n",
    "    X_img = np.repeat(X_img, 3, 0)\n",
    "    X_que = question_train[start:end,:]\n",
    "    y     = answer_train[start:end,:]\n",
    "    loss, acc = model_1.train_on_batch([X_img, X_que], y)\n",
    "    \n",
    "    epoch_loss.append(loss)\n",
    "    epoch_acc.append(acc)\n",
    "    \n",
    "    print(\"cost: %f, acc: %f\" % (loss, acc))\n",
    "    print(\"cost average: %f, acc average: %f\" % (np.mean(epoch_loss), np.mean(epoch_acc)))\n",
    "\n",
    "model_1.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ctiMZEYDaXg6",
    "outputId": "81482f83-10f9-47d8-de92-1b3965206192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121512/121512 [==============================] - 18s 147us/sample - loss: 1.9348 - acc: 0.4202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9347672676567627, 0.4202054]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on validation data\n",
    "from tensorflow.keras.models import load_model\n",
    "import h5py\n",
    "import numpy as np\n",
    "h5_img = h5py.File('full_img_features_val.h5', 'r')\n",
    "img_features_val = h5_img['full_img_features_val'][:]\n",
    "\n",
    "h5_ans = h5py.File('full_answers_val.h5', 'r')\n",
    "answer_val   = h5_ans['full_answers_val'][:]\n",
    "h5_ans.close()\n",
    "\n",
    "h5_que = h5py.File('full_question_val_tokenize.h5', 'r')\n",
    "question_val = h5_que['full_question_val_tokenize'][:]\n",
    "\n",
    "img_features_val = np.repeat(img_features_val, 3, 0)\n",
    "model_1 = load_model('model_1.h5')\n",
    "model_1.evaluate([img_features_val, question_val], answer_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYm9D9o1Ltml"
   },
   "source": [
    "## Python Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "orb4498T-uDX",
    "outputId": "617887d1-34cd-4f46-ec0c-8cfdef70e99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 2.9407 - acc: 0.2233 - val_loss: 2.6539 - val_acc: 0.2417\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 2.2531 - acc: 0.3266 - val_loss: 2.0103 - val_acc: 0.3638\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.9436 - acc: 0.3650 - val_loss: 1.8755 - val_acc: 0.3764\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.8563 - acc: 0.3745 - val_loss: 1.8290 - val_acc: 0.3826\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.8185 - acc: 0.3804 - val_loss: 1.8183 - val_acc: 0.3853\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.7933 - acc: 0.3854 - val_loss: 1.7899 - val_acc: 0.3885\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.7718 - acc: 0.3898 - val_loss: 1.7733 - val_acc: 0.3906\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.7522 - acc: 0.3956 - val_loss: 1.7633 - val_acc: 0.3941\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.7363 - acc: 0.3997 - val_loss: 1.7529 - val_acc: 0.3968\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.7198 - acc: 0.4054 - val_loss: 1.7517 - val_acc: 0.3992\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.7046 - acc: 0.4087 - val_loss: 1.7335 - val_acc: 0.4023\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.6894 - acc: 0.4138 - val_loss: 1.7292 - val_acc: 0.4049\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.6754 - acc: 0.4184 - val_loss: 1.7223 - val_acc: 0.4062\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.6613 - acc: 0.4233 - val_loss: 1.7158 - val_acc: 0.4101\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.6456 - acc: 0.4289 - val_loss: 1.7131 - val_acc: 0.4126\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.6317 - acc: 0.4330 - val_loss: 1.7055 - val_acc: 0.4147\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.6190 - acc: 0.4383 - val_loss: 1.7053 - val_acc: 0.4166\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.6053 - acc: 0.4427 - val_loss: 1.7118 - val_acc: 0.4143\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.5880 - acc: 0.4478 - val_loss: 1.7056 - val_acc: 0.4172\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.5736 - acc: 0.4524 - val_loss: 1.7021 - val_acc: 0.4186\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.5582 - acc: 0.4580 - val_loss: 1.6914 - val_acc: 0.4226\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.5415 - acc: 0.4632 - val_loss: 1.6904 - val_acc: 0.4247\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.5268 - acc: 0.4681 - val_loss: 1.6934 - val_acc: 0.4225\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.5099 - acc: 0.4735 - val_loss: 1.7048 - val_acc: 0.4197\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 93s 37ms/step - loss: 1.4955 - acc: 0.4794 - val_loss: 1.6904 - val_acc: 0.4250\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.4800 - acc: 0.4827 - val_loss: 1.6873 - val_acc: 0.4279\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.4620 - acc: 0.4899 - val_loss: 1.6947 - val_acc: 0.4258\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.4455 - acc: 0.4958 - val_loss: 1.6954 - val_acc: 0.4266\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.4288 - acc: 0.5002 - val_loss: 1.7057 - val_acc: 0.4257\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.4135 - acc: 0.5041 - val_loss: 1.7109 - val_acc: 0.4240\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.3965 - acc: 0.5113 - val_loss: 1.7188 - val_acc: 0.4248\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.3776 - acc: 0.5180 - val_loss: 1.7174 - val_acc: 0.4254\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.3590 - acc: 0.5234 - val_loss: 1.7131 - val_acc: 0.4251\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.3424 - acc: 0.5270 - val_loss: 1.7283 - val_acc: 0.4204\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 90s 36ms/step - loss: 1.3253 - acc: 0.5336 - val_loss: 1.7295 - val_acc: 0.4224\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 90s 36ms/step - loss: 1.3084 - acc: 0.5389 - val_loss: 1.7358 - val_acc: 0.4215\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2894 - acc: 0.5457 - val_loss: 1.7425 - val_acc: 0.4228\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2707 - acc: 0.5529 - val_loss: 1.7480 - val_acc: 0.4215\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2551 - acc: 0.5559 - val_loss: 1.7546 - val_acc: 0.4205\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2390 - acc: 0.5623 - val_loss: 1.7721 - val_acc: 0.4194\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2225 - acc: 0.5668 - val_loss: 1.7713 - val_acc: 0.4225\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.2068 - acc: 0.5734 - val_loss: 1.7802 - val_acc: 0.4177\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.1909 - acc: 0.5781 - val_loss: 1.7912 - val_acc: 0.4194\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.1732 - acc: 0.5833 - val_loss: 1.7903 - val_acc: 0.4168\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.1588 - acc: 0.5887 - val_loss: 1.7929 - val_acc: 0.4157\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 93s 37ms/step - loss: 1.1438 - acc: 0.5918 - val_loss: 1.7969 - val_acc: 0.4213\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.1273 - acc: 0.5977 - val_loss: 1.8049 - val_acc: 0.4189\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.1126 - acc: 0.6035 - val_loss: 1.8128 - val_acc: 0.4180\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 92s 37ms/step - loss: 1.0996 - acc: 0.6081 - val_loss: 1.8181 - val_acc: 0.4201\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 91s 37ms/step - loss: 1.0852 - acc: 0.6113 - val_loss: 1.8322 - val_acc: 0.4193\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "\n",
    "def generator(answer, img, question, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_img =      HDF5Matrix(img, 'image_train')\n",
    "        x_question = HDF5Matrix(question, 'full_question_train_tokenize')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_question.end\n",
    "        \n",
    "    elif types == 'test':\n",
    "        x_img =      HDF5Matrix(img, 'image_val')\n",
    "        x_question = HDF5Matrix(question, 'full_question_val_tokenize')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_question.end\n",
    "        \n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield [x_img[idx:end], x_question[idx:end]], y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_1.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmrVlI0po5KS"
   },
   "outputs": [],
   "source": [
    "model_1.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_z2iVVy4VRI"
   },
   "source": [
    "# 1 GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "R8Lh60MDzerA",
    "outputId": "94d68e0a-edb2-49af-e523-5f84bb5bf99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n",
      "Creating image model...\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)            (None, 512)          1250304     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           cu_dnngru[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         525312      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1001)         2051049     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1001)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         1003002     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 13,857,995\n",
      "Trainable params: 9,024,995\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 2.9101 - acc: 0.2285 - val_loss: 2.5403 - val_acc: 0.2777\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 2.2641 - acc: 0.3250 - val_loss: 2.0072 - val_acc: 0.3616\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.9668 - acc: 0.3617 - val_loss: 1.8956 - val_acc: 0.3772\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.8719 - acc: 0.3735 - val_loss: 1.8402 - val_acc: 0.3832\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.8265 - acc: 0.3807 - val_loss: 1.8160 - val_acc: 0.3872\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7990 - acc: 0.3880 - val_loss: 1.7964 - val_acc: 0.3899\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7750 - acc: 0.3921 - val_loss: 1.7774 - val_acc: 0.3924\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7564 - acc: 0.3978 - val_loss: 1.7689 - val_acc: 0.3961\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7375 - acc: 0.4024 - val_loss: 1.7529 - val_acc: 0.4010\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7211 - acc: 0.4088 - val_loss: 1.7571 - val_acc: 0.4041\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.7038 - acc: 0.4131 - val_loss: 1.7414 - val_acc: 0.4060\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.6890 - acc: 0.4193 - val_loss: 1.7276 - val_acc: 0.4110\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6740 - acc: 0.4237 - val_loss: 1.7256 - val_acc: 0.4120\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6600 - acc: 0.4286 - val_loss: 1.7201 - val_acc: 0.4157\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6460 - acc: 0.4335 - val_loss: 1.7102 - val_acc: 0.4186\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6338 - acc: 0.4365 - val_loss: 1.7097 - val_acc: 0.4191\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6185 - acc: 0.4421 - val_loss: 1.7074 - val_acc: 0.4209\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6067 - acc: 0.4441 - val_loss: 1.7144 - val_acc: 0.4182\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5917 - acc: 0.4496 - val_loss: 1.7075 - val_acc: 0.4214\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5779 - acc: 0.4537 - val_loss: 1.7034 - val_acc: 0.4228\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5627 - acc: 0.4595 - val_loss: 1.6925 - val_acc: 0.4266\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5503 - acc: 0.4635 - val_loss: 1.6913 - val_acc: 0.4286\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5331 - acc: 0.4678 - val_loss: 1.6936 - val_acc: 0.4278\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5207 - acc: 0.4722 - val_loss: 1.7065 - val_acc: 0.4238\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.5057 - acc: 0.4775 - val_loss: 1.6940 - val_acc: 0.4278\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4880 - acc: 0.4831 - val_loss: 1.6898 - val_acc: 0.4296\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4740 - acc: 0.4874 - val_loss: 1.6945 - val_acc: 0.4300\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4580 - acc: 0.4926 - val_loss: 1.6971 - val_acc: 0.4302\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4411 - acc: 0.4972 - val_loss: 1.7135 - val_acc: 0.4259\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4250 - acc: 0.5025 - val_loss: 1.7201 - val_acc: 0.4257\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.4069 - acc: 0.5084 - val_loss: 1.7181 - val_acc: 0.4283\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3894 - acc: 0.5152 - val_loss: 1.7109 - val_acc: 0.4295\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3731 - acc: 0.5198 - val_loss: 1.7148 - val_acc: 0.4284\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3581 - acc: 0.5245 - val_loss: 1.7154 - val_acc: 0.4270\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3395 - acc: 0.5319 - val_loss: 1.7318 - val_acc: 0.4236\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3220 - acc: 0.5358 - val_loss: 1.7289 - val_acc: 0.4270\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.3046 - acc: 0.5413 - val_loss: 1.7515 - val_acc: 0.4248\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 75s 30ms/step - loss: 1.2899 - acc: 0.5461 - val_loss: 1.7449 - val_acc: 0.4248\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.2705 - acc: 0.5523 - val_loss: 1.7521 - val_acc: 0.4227\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.2551 - acc: 0.5580 - val_loss: 1.7597 - val_acc: 0.4233\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.2381 - acc: 0.5627 - val_loss: 1.7765 - val_acc: 0.4239\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.2232 - acc: 0.5681 - val_loss: 1.7732 - val_acc: 0.4236\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.2049 - acc: 0.5732 - val_loss: 1.7818 - val_acc: 0.4172\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.1891 - acc: 0.5788 - val_loss: 1.7831 - val_acc: 0.4217\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.1730 - acc: 0.5840 - val_loss: 1.7881 - val_acc: 0.4190\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.1595 - acc: 0.5888 - val_loss: 1.7923 - val_acc: 0.4219\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.1429 - acc: 0.5934 - val_loss: 1.8040 - val_acc: 0.4216\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.1286 - acc: 0.5971 - val_loss: 1.8161 - val_acc: 0.4206\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.1154 - acc: 0.6017 - val_loss: 1.8176 - val_acc: 0.4231\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 77s 31ms/step - loss: 1.1031 - acc: 0.6071 - val_loss: 1.8248 - val_acc: 0.4226\n"
     ]
    }
   ],
   "source": [
    "# 1 gru\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, CuDNNGRU, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "lstm_1 = CuDNNGRU(units=512, return_sequences=False)(x)\n",
    "dropout__ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout__ques_1)\n",
    "print (\"Creating image model...\")\n",
    "\n",
    "image_input = Input(shape=(4096, ) )\n",
    ".dense_img_1 = Dense(1024,  activation='relu')(image_input)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_1, dense_ques_1])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_2 = Model(inputs=[image_input, question_input], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_2.summary()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_2.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "model_2.save(\"model_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tmzPfw2m4ayU"
   },
   "source": [
    "# 1 Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q2ja4mHzzqKL",
    "outputId": "ba7f573b-6d38-458b-f180-8c2a7518fe7a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 20:06:11.399725 140418270943104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0701 20:06:11.401146 140418270943104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0701 20:06:11.405744 140418270943104 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n",
      "Creating image model...\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1024)         3334144     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1049600     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1001)         2051049     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1001)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         1003002     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,466,123\n",
      "Trainable params: 11,633,123\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 2.9472 - acc: 0.2231 - val_loss: 2.6913 - val_acc: 0.2400\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 2.2968 - acc: 0.3172 - val_loss: 2.0480 - val_acc: 0.3584\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.9540 - acc: 0.3637 - val_loss: 1.8890 - val_acc: 0.3749\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.8628 - acc: 0.3733 - val_loss: 1.8347 - val_acc: 0.3815\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.8225 - acc: 0.3788 - val_loss: 1.8153 - val_acc: 0.3851\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.7958 - acc: 0.3845 - val_loss: 1.7980 - val_acc: 0.3874\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.7755 - acc: 0.3889 - val_loss: 1.7785 - val_acc: 0.3897\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.7561 - acc: 0.3944 - val_loss: 1.7719 - val_acc: 0.3913\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.7373 - acc: 0.3996 - val_loss: 1.7566 - val_acc: 0.3960\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.7226 - acc: 0.4034 - val_loss: 1.7498 - val_acc: 0.3995\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.7073 - acc: 0.4090 - val_loss: 1.7380 - val_acc: 0.4004\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.6926 - acc: 0.4130 - val_loss: 1.7344 - val_acc: 0.4030\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.6776 - acc: 0.4175 - val_loss: 1.7288 - val_acc: 0.4055\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.6629 - acc: 0.4240 - val_loss: 1.7213 - val_acc: 0.4091\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.6491 - acc: 0.4284 - val_loss: 1.7169 - val_acc: 0.4114\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.6330 - acc: 0.4328 - val_loss: 1.7154 - val_acc: 0.4110\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.6198 - acc: 0.4368 - val_loss: 1.7081 - val_acc: 0.4145\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.6056 - acc: 0.4418 - val_loss: 1.7161 - val_acc: 0.4125\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5904 - acc: 0.4473 - val_loss: 1.7078 - val_acc: 0.4159\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.5768 - acc: 0.4521 - val_loss: 1.7010 - val_acc: 0.4182\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.5604 - acc: 0.4574 - val_loss: 1.6940 - val_acc: 0.4217\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.5461 - acc: 0.4614 - val_loss: 1.6912 - val_acc: 0.4236\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5281 - acc: 0.4686 - val_loss: 1.6924 - val_acc: 0.4238\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.5146 - acc: 0.4729 - val_loss: 1.7080 - val_acc: 0.4204\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.4966 - acc: 0.4779 - val_loss: 1.6934 - val_acc: 0.4249\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.4818 - acc: 0.4833 - val_loss: 1.6914 - val_acc: 0.4265\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.4640 - acc: 0.4899 - val_loss: 1.6979 - val_acc: 0.4254\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.4476 - acc: 0.4955 - val_loss: 1.6981 - val_acc: 0.4257\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.4302 - acc: 0.5005 - val_loss: 1.7129 - val_acc: 0.4227\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.4127 - acc: 0.5054 - val_loss: 1.7084 - val_acc: 0.4249\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.3932 - acc: 0.5120 - val_loss: 1.7135 - val_acc: 0.4267\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.3757 - acc: 0.5179 - val_loss: 1.7150 - val_acc: 0.4252\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.3580 - acc: 0.5239 - val_loss: 1.7117 - val_acc: 0.4261\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.3436 - acc: 0.5295 - val_loss: 1.7199 - val_acc: 0.4228\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.3236 - acc: 0.5343 - val_loss: 1.7255 - val_acc: 0.4225\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.3089 - acc: 0.5399 - val_loss: 1.7365 - val_acc: 0.4205\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.2897 - acc: 0.5474 - val_loss: 1.7397 - val_acc: 0.4251\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.2712 - acc: 0.5528 - val_loss: 1.7511 - val_acc: 0.4228\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.2553 - acc: 0.5575 - val_loss: 1.7583 - val_acc: 0.4182\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.2392 - acc: 0.5631 - val_loss: 1.7672 - val_acc: 0.4194\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.2224 - acc: 0.5672 - val_loss: 1.7705 - val_acc: 0.4206\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.2050 - acc: 0.5735 - val_loss: 1.7826 - val_acc: 0.4171\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.1883 - acc: 0.5790 - val_loss: 1.7847 - val_acc: 0.4177\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.1745 - acc: 0.5837 - val_loss: 1.7820 - val_acc: 0.4175\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.1571 - acc: 0.5897 - val_loss: 1.7843 - val_acc: 0.4162\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.1441 - acc: 0.5932 - val_loss: 1.7913 - val_acc: 0.4209\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.1286 - acc: 0.5983 - val_loss: 1.8054 - val_acc: 0.4186\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.1121 - acc: 0.6032 - val_loss: 1.8105 - val_acc: 0.4183\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.1001 - acc: 0.6081 - val_loss: 1.8225 - val_acc: 0.4204\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.0854 - acc: 0.6123 - val_loss: 1.8353 - val_acc: 0.4193\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNLSTM, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "lstm_1 = Bidirectional(CuDNNLSTM(units=512, return_sequences=False))(x)\n",
    "dropout__ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout__ques_1)\n",
    "print (\"Creating image model...\")\n",
    "\n",
    "image_input = Input(shape=(4096, ) )\n",
    "dense_img_1 = Dense(1024,  activation='relu')(image_input)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_1, dense_ques_1])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_3 = Model(inputs=[image_input, question_input], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_3.summary()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_3.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_3.save(\"model_3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtL4U_nW4gLI"
   },
   "source": [
    "# 1 Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "iIfjhD7G23jY",
    "outputId": "378c9e09-e249-4176-c772-2b3f6f1ec50b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 12:30:26.117719 139776743778176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 12:30:27.758324 139776743778176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 12:30:27.767980 139776743778176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 12:30:27.769485 139776743778176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0703 12:30:27.771324 139776743778176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1024)         2500608     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1024)         0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         1049600     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2048)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1001)         2051049     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1001)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         1003002     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 15,632,587\n",
      "Trainable params: 10,799,587\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 104s 42ms/step - loss: 2.9273 - acc: 0.2250 - val_loss: 2.6202 - val_acc: 0.2561\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 2.3405 - acc: 0.3125 - val_loss: 2.0877 - val_acc: 0.3488\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.9853 - acc: 0.3595 - val_loss: 1.8988 - val_acc: 0.3767\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.8815 - acc: 0.3721 - val_loss: 1.8442 - val_acc: 0.3823\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.8353 - acc: 0.3796 - val_loss: 1.8255 - val_acc: 0.3861\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.8050 - acc: 0.3845 - val_loss: 1.8008 - val_acc: 0.3892\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.7812 - acc: 0.3905 - val_loss: 1.7862 - val_acc: 0.3906\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.7610 - acc: 0.3956 - val_loss: 1.7758 - val_acc: 0.3932\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.7441 - acc: 0.4002 - val_loss: 1.7563 - val_acc: 0.3983\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.7257 - acc: 0.4057 - val_loss: 1.7555 - val_acc: 0.4012\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.7110 - acc: 0.4105 - val_loss: 1.7425 - val_acc: 0.4067\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.6949 - acc: 0.4164 - val_loss: 1.7292 - val_acc: 0.4095\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.6788 - acc: 0.4221 - val_loss: 1.7313 - val_acc: 0.4089\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 1.6668 - acc: 0.4247 - val_loss: 1.7229 - val_acc: 0.4129\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.6517 - acc: 0.4304 - val_loss: 1.7139 - val_acc: 0.4169\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.6381 - acc: 0.4346 - val_loss: 1.7141 - val_acc: 0.4163\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 1.6237 - acc: 0.4400 - val_loss: 1.7065 - val_acc: 0.4198\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.6107 - acc: 0.4437 - val_loss: 1.7244 - val_acc: 0.4152\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 98s 39ms/step - loss: 1.5978 - acc: 0.4468 - val_loss: 1.7186 - val_acc: 0.4170\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5821 - acc: 0.4529 - val_loss: 1.7046 - val_acc: 0.4217\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5684 - acc: 0.4569 - val_loss: 1.6955 - val_acc: 0.4256\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5540 - acc: 0.4621 - val_loss: 1.7007 - val_acc: 0.4244\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5392 - acc: 0.4663 - val_loss: 1.6998 - val_acc: 0.4245\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5242 - acc: 0.4706 - val_loss: 1.7132 - val_acc: 0.4225\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.5096 - acc: 0.4761 - val_loss: 1.6983 - val_acc: 0.4279\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.4943 - acc: 0.4812 - val_loss: 1.6988 - val_acc: 0.4281\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 97s 39ms/step - loss: 1.4763 - acc: 0.4865 - val_loss: 1.7028 - val_acc: 0.4271\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.4630 - acc: 0.4898 - val_loss: 1.7018 - val_acc: 0.4288\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.4453 - acc: 0.4971 - val_loss: 1.7103 - val_acc: 0.4264\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 96s 39ms/step - loss: 1.4268 - acc: 0.5017 - val_loss: 1.7136 - val_acc: 0.4262\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.4117 - acc: 0.5060 - val_loss: 1.7183 - val_acc: 0.4274\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.3939 - acc: 0.5125 - val_loss: 1.7212 - val_acc: 0.4252\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.3766 - acc: 0.5183 - val_loss: 1.7188 - val_acc: 0.4276\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.3623 - acc: 0.5238 - val_loss: 1.7254 - val_acc: 0.4252\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.3424 - acc: 0.5299 - val_loss: 1.7322 - val_acc: 0.4249\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.3269 - acc: 0.5336 - val_loss: 1.7527 - val_acc: 0.4240\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.3081 - acc: 0.5416 - val_loss: 1.7495 - val_acc: 0.4243\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.2900 - acc: 0.5464 - val_loss: 1.7495 - val_acc: 0.4234\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 95s 38ms/step - loss: 1.2729 - acc: 0.5510 - val_loss: 1.7548 - val_acc: 0.4225\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.2606 - acc: 0.5565 - val_loss: 1.7674 - val_acc: 0.4189\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.2409 - acc: 0.5614 - val_loss: 1.7816 - val_acc: 0.4217\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.2255 - acc: 0.5673 - val_loss: 1.7814 - val_acc: 0.4188\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.2075 - acc: 0.5723 - val_loss: 1.7888 - val_acc: 0.4190\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.1939 - acc: 0.5780 - val_loss: 1.7976 - val_acc: 0.4143\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 93s 37ms/step - loss: 1.1774 - acc: 0.5820 - val_loss: 1.7948 - val_acc: 0.4166\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.1621 - acc: 0.5874 - val_loss: 1.8049 - val_acc: 0.4207\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 93s 37ms/step - loss: 1.1468 - acc: 0.5936 - val_loss: 1.8089 - val_acc: 0.4175\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 93s 38ms/step - loss: 1.1337 - acc: 0.5966 - val_loss: 1.8156 - val_acc: 0.4173\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 93s 37ms/step - loss: 1.1177 - acc: 0.6033 - val_loss: 1.8246 - val_acc: 0.4179\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 94s 38ms/step - loss: 1.1086 - acc: 0.6053 - val_loss: 1.8344 - val_acc: 0.4192\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional GRU\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNGRU, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "\n",
    "dropout_rate = 0.5\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "#question_input = Input(tensor=question)\n",
    "\n",
    "#x = Embedding(output_dim=512, input_dim=125, input_length=25)(question_input)\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "lstm_1 = Bidirectional(CuDNNGRU(units=512, return_sequences=False))(x)\n",
    "dropout__ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "#lstm_2 = CuDNNLSTM(units=512, return_sequences=False)(dropout__ques_1)\n",
    "#dropout__ques_2 = Dropout(dropout_rate)(lstm_2)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout__ques_1)\n",
    "print (\"Creating image model...\")\n",
    "\n",
    "image_input = Input(shape=(4096, ) )\n",
    "#reshape = reshape((4096,))(image_input)\n",
    "dense_img_1 = Dense(1024,  activation='relu')(image_input)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_1, dense_ques_1])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_4 = Model(inputs=[image_input, question_input], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_4.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_4.summary()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_4.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_4.save(\"model_4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y8tf7jZ4k0E"
   },
   "source": [
    "# Bags of Words implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qO9esEfI3IkT",
    "outputId": "d0a9044e-c410-45ca-f9e1-edae5a949bd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4121)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              4220928   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1001)              1026025   \n",
      "=================================================================\n",
      "Total params: 7,346,153\n",
      "Trainable params: 7,346,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 3.2187 - acc: 0.2410 - val_loss: 2.7576 - val_acc: 0.2618\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.7465 - acc: 0.2583 - val_loss: 2.6802 - val_acc: 0.2663\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.7104 - acc: 0.2624 - val_loss: 2.6251 - val_acc: 0.2726\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6691 - acc: 0.2671 - val_loss: 2.6063 - val_acc: 0.2714\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6532 - acc: 0.2690 - val_loss: 2.5946 - val_acc: 0.2696\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6456 - acc: 0.2698 - val_loss: 2.5808 - val_acc: 0.2807\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6387 - acc: 0.2709 - val_loss: 2.5684 - val_acc: 0.2796\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6325 - acc: 0.2720 - val_loss: 2.5760 - val_acc: 0.2780\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6320 - acc: 0.2711 - val_loss: 2.5635 - val_acc: 0.2792\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6305 - acc: 0.2737 - val_loss: 2.5714 - val_acc: 0.2803\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6244 - acc: 0.2740 - val_loss: 2.5624 - val_acc: 0.2798\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6236 - acc: 0.2733 - val_loss: 2.5733 - val_acc: 0.2819\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6195 - acc: 0.2749 - val_loss: 2.5482 - val_acc: 0.2822\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6168 - acc: 0.2751 - val_loss: 2.5486 - val_acc: 0.2820\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6087 - acc: 0.2752 - val_loss: 2.5434 - val_acc: 0.2821\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6057 - acc: 0.2777 - val_loss: 2.5316 - val_acc: 0.2852\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6017 - acc: 0.2771 - val_loss: 2.5325 - val_acc: 0.2870\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5988 - acc: 0.2771 - val_loss: 2.5414 - val_acc: 0.2802\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.6000 - acc: 0.2782 - val_loss: 2.5437 - val_acc: 0.2823\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5961 - acc: 0.2779 - val_loss: 2.5281 - val_acc: 0.2867\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5963 - acc: 0.2777 - val_loss: 2.5247 - val_acc: 0.2856\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5934 - acc: 0.2792 - val_loss: 2.5280 - val_acc: 0.2856\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5952 - acc: 0.2785 - val_loss: 2.5312 - val_acc: 0.2843\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5877 - acc: 0.2787 - val_loss: 2.5235 - val_acc: 0.2852\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5876 - acc: 0.2784 - val_loss: 2.5244 - val_acc: 0.2864\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5859 - acc: 0.2793 - val_loss: 2.5130 - val_acc: 0.2891\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5892 - acc: 0.2789 - val_loss: 2.5269 - val_acc: 0.2831\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5862 - acc: 0.2801 - val_loss: 2.5315 - val_acc: 0.2836\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5892 - acc: 0.2797 - val_loss: 2.5143 - val_acc: 0.2854\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5802 - acc: 0.2808 - val_loss: 2.5069 - val_acc: 0.2867\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5796 - acc: 0.2805 - val_loss: 2.5110 - val_acc: 0.2925\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5767 - acc: 0.2814 - val_loss: 2.5033 - val_acc: 0.2899\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5793 - acc: 0.2821 - val_loss: 2.5093 - val_acc: 0.2919\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5771 - acc: 0.2821 - val_loss: 2.5064 - val_acc: 0.2928\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5722 - acc: 0.2823 - val_loss: 2.5083 - val_acc: 0.2926\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5774 - acc: 0.2821 - val_loss: 2.5051 - val_acc: 0.2917\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5842 - acc: 0.2811 - val_loss: 2.5032 - val_acc: 0.2924\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5754 - acc: 0.2818 - val_loss: 2.5053 - val_acc: 0.2910\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5749 - acc: 0.2821 - val_loss: 2.5133 - val_acc: 0.2861\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5727 - acc: 0.2822 - val_loss: 2.5011 - val_acc: 0.2907\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5773 - acc: 0.2814 - val_loss: 2.5103 - val_acc: 0.2918\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5693 - acc: 0.2837 - val_loss: 2.5025 - val_acc: 0.2924\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5714 - acc: 0.2814 - val_loss: 2.5036 - val_acc: 0.2886\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5694 - acc: 0.2835 - val_loss: 2.4961 - val_acc: 0.2937\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5645 - acc: 0.2843 - val_loss: 2.4966 - val_acc: 0.2907\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5667 - acc: 0.2834 - val_loss: 2.5050 - val_acc: 0.2893\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5659 - acc: 0.2839 - val_loss: 2.4903 - val_acc: 0.2938\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5669 - acc: 0.2837 - val_loss: 2.5166 - val_acc: 0.2914\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5702 - acc: 0.2825 - val_loss: 2.4987 - val_acc: 0.2914\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 80s 32ms/step - loss: 2.5599 - acc: 0.2840 - val_loss: 2.4907 - val_acc: 0.2928\n"
     ]
    }
   ],
   "source": [
    "# Bags of word implementation\n",
    "# Bags of word implementation\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout,  Flatten, Embedding, Conv1D, Input\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "word_dim = 25\n",
    "img_dim = 4096\n",
    "input_feats = Input(shape=(word_dim + img_dim, ))\n",
    "\n",
    "#x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], \n",
    "#              input_length = img_dim + word_dim, trainable = False)(input_feats)\n",
    "\n",
    "dense_1 = Dense(1024, activation='relu')(input_feats)\n",
    "dropout_1 = Dropout(dropout_rate)(dense_1)\n",
    "dense_2 = Dense(1024,  activation='relu')(dropout_1)\n",
    "dropout_2 = Dropout(dropout_rate)(dense_2)\n",
    "\n",
    "dense_3 = Dense(1024,  activation='relu')(dropout_2)\n",
    "dropout_3 = Dropout(dropout_rate)(dense_3)\n",
    "\n",
    "dense_4 = Dense(1001, activation = \"softmax\")(dropout_3)\n",
    "\n",
    "model_5 = Model(inputs = input_feats, outputs = dense_4)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_5.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "model_5.summary()\n",
    "\n",
    "def bow_generator(answer, img, question, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_img =      HDF5Matrix(img, 'image_train')\n",
    "        x_question = HDF5Matrix(question, 'full_question_train_tokenize')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_question.end\n",
    "    \n",
    "    elif types == 'test':\n",
    "        x_img =      HDF5Matrix(img, 'image_val')\n",
    "        x_question = HDF5Matrix(question, 'full_question_val_tokenize')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_question.end\n",
    "    \n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield np.hstack((x_img[idx:end], x_question[idx:end])), y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = bow_generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  bow_generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_5.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_5.save(\"model_5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgYZuqjX4pLS"
   },
   "source": [
    "# CNN proposed architecute on textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V9cnqzvt3TvO",
    "outputId": "b0d36f98-e067-4961-9df0-a434b88d655b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n",
      "Creating image model...\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 25, 128)      38528       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 25, 128)      0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 25, 128)      16512       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 25, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 25, 128)      16512       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 25, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3200)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          409728      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         132096      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2048)         0           dense_2[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         2051049     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1001)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1001)         1003002     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,695,755\n",
      "Trainable params: 7,862,755\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 82s 33ms/step - loss: 2.9619 - acc: 0.2220 - val_loss: 2.7323 - val_acc: 0.2395\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 2.2621 - acc: 0.3198 - val_loss: 1.9302 - val_acc: 0.3688\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.8891 - acc: 0.3668 - val_loss: 1.8422 - val_acc: 0.3802\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.8306 - acc: 0.3749 - val_loss: 1.8098 - val_acc: 0.3834\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.8019 - acc: 0.3814 - val_loss: 1.8065 - val_acc: 0.3864\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.7787 - acc: 0.3866 - val_loss: 1.7826 - val_acc: 0.3902\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.7563 - acc: 0.3943 - val_loss: 1.7655 - val_acc: 0.3935\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.7363 - acc: 0.4002 - val_loss: 1.7557 - val_acc: 0.3967\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.7187 - acc: 0.4056 - val_loss: 1.7422 - val_acc: 0.4025\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.7014 - acc: 0.4118 - val_loss: 1.7510 - val_acc: 0.4030\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6851 - acc: 0.4185 - val_loss: 1.7402 - val_acc: 0.3998\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6693 - acc: 0.4240 - val_loss: 1.7204 - val_acc: 0.4093\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6542 - acc: 0.4291 - val_loss: 1.7182 - val_acc: 0.4153\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6373 - acc: 0.4376 - val_loss: 1.7100 - val_acc: 0.4195\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6204 - acc: 0.4426 - val_loss: 1.7004 - val_acc: 0.4230\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.6020 - acc: 0.4482 - val_loss: 1.6999 - val_acc: 0.4252\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 82s 33ms/step - loss: 1.5861 - acc: 0.4553 - val_loss: 1.6922 - val_acc: 0.4282\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.5688 - acc: 0.4594 - val_loss: 1.6967 - val_acc: 0.4279\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.5501 - acc: 0.4661 - val_loss: 1.6874 - val_acc: 0.4312\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.5322 - acc: 0.4725 - val_loss: 1.6840 - val_acc: 0.4317\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.5131 - acc: 0.4795 - val_loss: 1.6753 - val_acc: 0.4336\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.4970 - acc: 0.4847 - val_loss: 1.6866 - val_acc: 0.4324\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.4762 - acc: 0.4910 - val_loss: 1.6739 - val_acc: 0.4383\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.4564 - acc: 0.4984 - val_loss: 1.6977 - val_acc: 0.4330\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.4365 - acc: 0.5051 - val_loss: 1.6732 - val_acc: 0.4409\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.4157 - acc: 0.5105 - val_loss: 1.6735 - val_acc: 0.4402\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.3964 - acc: 0.5175 - val_loss: 1.6751 - val_acc: 0.4417\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.3750 - acc: 0.5242 - val_loss: 1.6772 - val_acc: 0.4411\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.3568 - acc: 0.5299 - val_loss: 1.6996 - val_acc: 0.4394\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.3353 - acc: 0.5368 - val_loss: 1.6980 - val_acc: 0.4393\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.3152 - acc: 0.5444 - val_loss: 1.6927 - val_acc: 0.4404\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.2960 - acc: 0.5501 - val_loss: 1.7060 - val_acc: 0.4369\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.2720 - acc: 0.5588 - val_loss: 1.6976 - val_acc: 0.4406\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.2530 - acc: 0.5643 - val_loss: 1.7036 - val_acc: 0.4393\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.2307 - acc: 0.5717 - val_loss: 1.7152 - val_acc: 0.4387\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.2115 - acc: 0.5778 - val_loss: 1.7294 - val_acc: 0.4366\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.1900 - acc: 0.5835 - val_loss: 1.7354 - val_acc: 0.4386\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.1712 - acc: 0.5911 - val_loss: 1.7453 - val_acc: 0.4386\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.1502 - acc: 0.5975 - val_loss: 1.7478 - val_acc: 0.4341\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.1309 - acc: 0.6047 - val_loss: 1.7522 - val_acc: 0.4366\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.1127 - acc: 0.6100 - val_loss: 1.7747 - val_acc: 0.4393\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0946 - acc: 0.6162 - val_loss: 1.7707 - val_acc: 0.4402\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0754 - acc: 0.6222 - val_loss: 1.7805 - val_acc: 0.4366\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0573 - acc: 0.6282 - val_loss: 1.7864 - val_acc: 0.4359\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0385 - acc: 0.6342 - val_loss: 1.7964 - val_acc: 0.4361\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0226 - acc: 0.6403 - val_loss: 1.7901 - val_acc: 0.4413\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 1.0057 - acc: 0.6464 - val_loss: 1.8096 - val_acc: 0.4379\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 0.9884 - acc: 0.6522 - val_loss: 1.8211 - val_acc: 0.4389\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 0.9702 - acc: 0.6572 - val_loss: 1.8333 - val_acc: 0.4405\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 81s 33ms/step - loss: 0.9541 - acc: 0.6630 - val_loss: 1.8531 - val_acc: 0.4411\n"
     ]
    }
   ],
   "source": [
    "# CNN proposed architecture on textual data.\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNGRU, Flatten, Embedding, concatenate, Conv1D, Input, Embedding, MaxPooling1D\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "\n",
    "print (\"Creating text model...\")\n",
    "\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "\n",
    "l_cov1= Conv1D(128, 1, activation='relu')(x)\n",
    "l_pool1 = MaxPooling1D(1)(l_cov1)\n",
    "\n",
    "l_cov2 = Conv1D(128, 1, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(1)(l_cov2)\n",
    "\n",
    "l_cov3 = Conv1D(128, 1, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(1)(l_cov3)  # global max pooling\n",
    "\n",
    "l_flat = Flatten()(l_pool3)\n",
    "\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(l_dense)\n",
    "\n",
    "print (\"Creating image model...\")\n",
    "image_input = Input(shape=(4096, ) )\n",
    "dense_img_1 = Dense(1024,  activation='relu')(image_input)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_1, dense_ques_1])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_6 = Model(inputs=[image_input, question_input], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_6.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_6.summary()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_6.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_6.save(\"model_6.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lJajDlG4u9m"
   },
   "source": [
    "# 2 Vision LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zi37gMae3iEP",
    "outputId": "d0ccb148-6e12-42a8-c5c6-1484295981ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n",
      "Creating image model...\n",
      "creating image model 2\n",
      "Merging final model...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 25, 300)      4833000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          (None, 512)          1667072     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           cu_dnnlstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         4195328     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1024)         525312      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         4195328     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 3072)         0           dense_1[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 3072)         0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1001)         3076073     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1001)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1001)         1003002     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,495,115\n",
      "Trainable params: 14,662,115\n",
      "Non-trainable params: 4,833,000\n",
      "__________________________________________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 2.9607 - acc: 0.2200 - val_loss: 2.7812 - val_acc: 0.2389\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 98s 40ms/step - loss: 2.5219 - acc: 0.2773 - val_loss: 2.1368 - val_acc: 0.3502\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 99s 40ms/step - loss: 2.0348 - acc: 0.3547 - val_loss: 1.9419 - val_acc: 0.3737\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 1.8961 - acc: 0.3712 - val_loss: 1.8435 - val_acc: 0.3812\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.8319 - acc: 0.3777 - val_loss: 1.8228 - val_acc: 0.3859\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.7997 - acc: 0.3834 - val_loss: 1.8120 - val_acc: 0.3869\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.7782 - acc: 0.3874 - val_loss: 1.7895 - val_acc: 0.3894\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 1.7591 - acc: 0.3923 - val_loss: 1.7821 - val_acc: 0.3899\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.7415 - acc: 0.3964 - val_loss: 1.7670 - val_acc: 0.3929\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.7227 - acc: 0.4019 - val_loss: 1.7582 - val_acc: 0.3961\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.7068 - acc: 0.4066 - val_loss: 1.7456 - val_acc: 0.3966\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.6907 - acc: 0.4110 - val_loss: 1.7430 - val_acc: 0.3988\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.6748 - acc: 0.4170 - val_loss: 1.7403 - val_acc: 0.4012\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 1.6576 - acc: 0.4222 - val_loss: 1.7322 - val_acc: 0.4053\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.6424 - acc: 0.4271 - val_loss: 1.7307 - val_acc: 0.4063\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 103s 42ms/step - loss: 1.6261 - acc: 0.4330 - val_loss: 1.7290 - val_acc: 0.4081\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.6093 - acc: 0.4397 - val_loss: 1.7210 - val_acc: 0.4114\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 103s 42ms/step - loss: 1.5910 - acc: 0.4454 - val_loss: 1.7327 - val_acc: 0.4099\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.5717 - acc: 0.4517 - val_loss: 1.7241 - val_acc: 0.4124\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.5534 - acc: 0.4568 - val_loss: 1.7153 - val_acc: 0.4146\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.5335 - acc: 0.4641 - val_loss: 1.7109 - val_acc: 0.4183\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.5141 - acc: 0.4707 - val_loss: 1.7107 - val_acc: 0.4201\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.4932 - acc: 0.4781 - val_loss: 1.7188 - val_acc: 0.4185\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 104s 42ms/step - loss: 1.4715 - acc: 0.4859 - val_loss: 1.7328 - val_acc: 0.4174\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.4502 - acc: 0.4936 - val_loss: 1.7199 - val_acc: 0.4204\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.4280 - acc: 0.4992 - val_loss: 1.7203 - val_acc: 0.4227\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.4059 - acc: 0.5086 - val_loss: 1.7313 - val_acc: 0.4217\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.3843 - acc: 0.5142 - val_loss: 1.7350 - val_acc: 0.4209\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.3627 - acc: 0.5224 - val_loss: 1.7512 - val_acc: 0.4186\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.3372 - acc: 0.5303 - val_loss: 1.7530 - val_acc: 0.4197\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.3157 - acc: 0.5361 - val_loss: 1.7551 - val_acc: 0.4207\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.2899 - acc: 0.5457 - val_loss: 1.7544 - val_acc: 0.4183\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 100s 40ms/step - loss: 1.2677 - acc: 0.5526 - val_loss: 1.7611 - val_acc: 0.4188\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.2477 - acc: 0.5578 - val_loss: 1.7718 - val_acc: 0.4179\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.2240 - acc: 0.5671 - val_loss: 1.7900 - val_acc: 0.4156\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.1986 - acc: 0.5748 - val_loss: 1.8047 - val_acc: 0.4153\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 1.1781 - acc: 0.5810 - val_loss: 1.8059 - val_acc: 0.4171\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.1572 - acc: 0.5886 - val_loss: 1.8121 - val_acc: 0.4167\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.1357 - acc: 0.5965 - val_loss: 1.8294 - val_acc: 0.4076\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 103s 42ms/step - loss: 1.1108 - acc: 0.6040 - val_loss: 1.8393 - val_acc: 0.4136\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 1.0938 - acc: 0.6097 - val_loss: 1.8524 - val_acc: 0.4135\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.0747 - acc: 0.6165 - val_loss: 1.8563 - val_acc: 0.4124\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.0548 - acc: 0.6223 - val_loss: 1.8757 - val_acc: 0.4127\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 104s 42ms/step - loss: 1.0347 - acc: 0.6292 - val_loss: 1.8720 - val_acc: 0.4114\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 1.0200 - acc: 0.6343 - val_loss: 1.8758 - val_acc: 0.4148\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 101s 41ms/step - loss: 0.9970 - acc: 0.6416 - val_loss: 1.8894 - val_acc: 0.4122\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 103s 42ms/step - loss: 0.9821 - acc: 0.6486 - val_loss: 1.8903 - val_acc: 0.4127\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 0.9670 - acc: 0.6523 - val_loss: 1.9157 - val_acc: 0.4122\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 103s 41ms/step - loss: 0.9483 - acc: 0.6594 - val_loss: 1.9218 - val_acc: 0.4137\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 102s 41ms/step - loss: 0.9343 - acc: 0.6634 - val_loss: 1.9196 - val_acc: 0.4114\n"
     ]
    }
   ],
   "source": [
    "# 2 vis lstm.\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNLSTM, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "\n",
    "lstm_1 = CuDNNLSTM(units=512, return_sequences=False)(x)\n",
    "dropout_ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout_ques_1)\n",
    "\n",
    "print (\"Creating image model...\")\n",
    "image_input_model_1 = Input(shape=(4096, ) )\n",
    "dense_img_model_1 = Dense(1024,  activation='relu')(image_input_model_1)\n",
    "\n",
    "print(\"creating image model 2\")\n",
    "image_input_model_2 = Input(shape=(4096, ) )\n",
    "dense_img_model_2 = Dense(1024,  activation='relu')(image_input_model_2)\n",
    "\n",
    "print (\"Merging final model...\")\n",
    "\n",
    "concatenate_1 = concatenate([dense_img_model_1, dense_ques_1, dense_img_model_2])\n",
    "\n",
    "dropout_1 = Dropout(0.5)(concatenate_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_7 = Model(inputs=[image_input_model_1, question_input, image_input_model_2], outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_7.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_7.summary()\n",
    "\n",
    "def vis_2_generator(answer, img, question, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_img =      HDF5Matrix(img, 'image_train')\n",
    "        x_question = HDF5Matrix(question, 'full_question_train_tokenize')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_question.end\n",
    "      \n",
    "    elif types == 'test':\n",
    "        x_img =      HDF5Matrix(img, 'image_val')\n",
    "        x_question = HDF5Matrix(question, 'full_question_val_tokenize')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_question.end\n",
    "      \n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield [x_img[idx:end], x_question[idx:end], x_img[idx:end] ], y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = vis_2_generator(answer_train, img_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  vis_2_generator(answer_test,  img_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_7.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_7.save(\"model_7.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVdIXwIH4yvv"
   },
   "source": [
    "# Language Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qS0ElkNa3xFX",
    "outputId": "810139a0-147a-4d13-9c62-dd9ab76dd425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 25)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 25, 300)           4833000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1024)              3334144   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1001)              1026025   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1001)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1001)              1003002   \n",
      "=================================================================\n",
      "Total params: 11,245,771\n",
      "Trainable params: 6,412,771\n",
      "Non-trainable params: 4,833,000\n",
      "_________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 76s 31ms/step - loss: 2.7770 - acc: 0.2629 - val_loss: 2.1630 - val_acc: 0.3424\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 2.0576 - acc: 0.3397 - val_loss: 1.9475 - val_acc: 0.3531\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.9297 - acc: 0.3509 - val_loss: 1.9072 - val_acc: 0.3586\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.8870 - acc: 0.3583 - val_loss: 1.8679 - val_acc: 0.3612\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.8582 - acc: 0.3640 - val_loss: 1.8465 - val_acc: 0.3662\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.8361 - acc: 0.3685 - val_loss: 1.8185 - val_acc: 0.3724\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.8191 - acc: 0.3731 - val_loss: 1.7996 - val_acc: 0.3761\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.8035 - acc: 0.3776 - val_loss: 1.7917 - val_acc: 0.3813\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7911 - acc: 0.3812 - val_loss: 1.7753 - val_acc: 0.3856\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7799 - acc: 0.3848 - val_loss: 1.7761 - val_acc: 0.3881\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7684 - acc: 0.3883 - val_loss: 1.7563 - val_acc: 0.3924\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7606 - acc: 0.3914 - val_loss: 1.7509 - val_acc: 0.3940\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7521 - acc: 0.3934 - val_loss: 1.7519 - val_acc: 0.3918\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7449 - acc: 0.3955 - val_loss: 1.7404 - val_acc: 0.3973\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7387 - acc: 0.3985 - val_loss: 1.7327 - val_acc: 0.3998\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.7327 - acc: 0.3999 - val_loss: 1.7307 - val_acc: 0.3979\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7262 - acc: 0.4011 - val_loss: 1.7232 - val_acc: 0.4020\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7225 - acc: 0.4032 - val_loss: 1.7239 - val_acc: 0.4006\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7164 - acc: 0.4043 - val_loss: 1.7209 - val_acc: 0.4034\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7112 - acc: 0.4068 - val_loss: 1.7096 - val_acc: 0.4058\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7086 - acc: 0.4067 - val_loss: 1.7040 - val_acc: 0.4087\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7045 - acc: 0.4077 - val_loss: 1.6974 - val_acc: 0.4112\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.7009 - acc: 0.4096 - val_loss: 1.6973 - val_acc: 0.4104\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6968 - acc: 0.4110 - val_loss: 1.7038 - val_acc: 0.4063\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6933 - acc: 0.4116 - val_loss: 1.6917 - val_acc: 0.4123\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6902 - acc: 0.4129 - val_loss: 1.6851 - val_acc: 0.4165\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6874 - acc: 0.4131 - val_loss: 1.6851 - val_acc: 0.4149\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6856 - acc: 0.4132 - val_loss: 1.6806 - val_acc: 0.4153\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6824 - acc: 0.4152 - val_loss: 1.6856 - val_acc: 0.4145\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6798 - acc: 0.4160 - val_loss: 1.6802 - val_acc: 0.4161\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.6771 - acc: 0.4158 - val_loss: 1.6819 - val_acc: 0.4183\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.6742 - acc: 0.4181 - val_loss: 1.6777 - val_acc: 0.4186\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 71s 28ms/step - loss: 1.6723 - acc: 0.4192 - val_loss: 1.6721 - val_acc: 0.4214\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 71s 28ms/step - loss: 1.6703 - acc: 0.4183 - val_loss: 1.6705 - val_acc: 0.4201\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 70s 28ms/step - loss: 1.6676 - acc: 0.4201 - val_loss: 1.6701 - val_acc: 0.4204\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 71s 28ms/step - loss: 1.6665 - acc: 0.4201 - val_loss: 1.6693 - val_acc: 0.4220\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 70s 28ms/step - loss: 1.6633 - acc: 0.4209 - val_loss: 1.6720 - val_acc: 0.4216\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 71s 29ms/step - loss: 1.6620 - acc: 0.4213 - val_loss: 1.6742 - val_acc: 0.4203\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.6593 - acc: 0.4225 - val_loss: 1.6716 - val_acc: 0.4176\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6579 - acc: 0.4218 - val_loss: 1.6715 - val_acc: 0.4192\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6559 - acc: 0.4222 - val_loss: 1.6665 - val_acc: 0.4210\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 73s 29ms/step - loss: 1.6549 - acc: 0.4231 - val_loss: 1.6633 - val_acc: 0.4222\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6527 - acc: 0.4241 - val_loss: 1.6638 - val_acc: 0.4198\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6492 - acc: 0.4239 - val_loss: 1.6586 - val_acc: 0.4216\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6480 - acc: 0.4253 - val_loss: 1.6538 - val_acc: 0.4232\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6479 - acc: 0.4248 - val_loss: 1.6504 - val_acc: 0.4249\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6452 - acc: 0.4263 - val_loss: 1.6495 - val_acc: 0.4251\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6436 - acc: 0.4264 - val_loss: 1.6517 - val_acc: 0.4244\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6429 - acc: 0.4267 - val_loss: 1.6484 - val_acc: 0.4269\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 72s 29ms/step - loss: 1.6419 - acc: 0.4276 - val_loss: 1.6493 - val_acc: 0.4268\n"
     ]
    }
   ],
   "source": [
    "# language only model\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNLSTM, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "print (\"Creating text model...\")\n",
    "question_input = Input(shape=(25, ))\n",
    "\n",
    "x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], weights = [embedding_matrix], input_length=25, trainable = False)(question_input)\n",
    "lstm_1 = Bidirectional(CuDNNLSTM(units=512, return_sequences=False))(x)\n",
    "dropout__ques_1 = Dropout(dropout_rate)(lstm_1)\n",
    "\n",
    "dense_ques_1 = Dense(1024, activation='tanh')(dropout__ques_1)\n",
    "\n",
    "dropout_1 = Dropout(0.5)(dense_ques_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_8 = Model(inputs=question_input, outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_8.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_8.summary()\n",
    "\n",
    "def lang_generator(answer, question, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_question = HDF5Matrix(question, 'full_question_train_tokenize')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_question.end\n",
    "    elif types == 'test':\n",
    "        x_question = HDF5Matrix(question, 'full_question_val_tokenize')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_question.end\n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield x_question[idx:end], y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    question_train = 'full_question_train_tokenize.h5'\n",
    "    question_test =  'full_question_val_tokenize.h5'\n",
    "\n",
    "    train_generator = lang_generator(answer_train, question_train, batch_size, types = 'train')\n",
    "    test_generator =  lang_generator(answer_test,  question_test,  batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_8.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_8.save(\"model_8.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L3TrfGTWeaaq"
   },
   "source": [
    "# Image only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qGW2AVkkCVpc",
    "outputId": "48e67b9b-f049-4c71-f5bc-abbbc1538eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4096)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1001)              1026025   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1001)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1001)              1003002   \n",
      "=================================================================\n",
      "Total params: 6,224,355\n",
      "Trainable params: 6,224,355\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train samples: 248349, test samples: 121512\n",
      "Epoch 1/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.9821 - acc: 0.2208 - val_loss: 2.8097 - val_acc: 0.2388\n",
      "Epoch 2/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7975 - acc: 0.2361 - val_loss: 2.7661 - val_acc: 0.2394\n",
      "Epoch 3/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7627 - acc: 0.2376 - val_loss: 2.7513 - val_acc: 0.2393\n",
      "Epoch 4/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7443 - acc: 0.2388 - val_loss: 2.7457 - val_acc: 0.2388\n",
      "Epoch 5/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7316 - acc: 0.2386 - val_loss: 2.7518 - val_acc: 0.2389\n",
      "Epoch 6/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7221 - acc: 0.2383 - val_loss: 2.7386 - val_acc: 0.2383\n",
      "Epoch 7/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7133 - acc: 0.2390 - val_loss: 2.7425 - val_acc: 0.2387\n",
      "Epoch 8/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.7057 - acc: 0.2391 - val_loss: 2.7370 - val_acc: 0.2385\n",
      "Epoch 9/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6989 - acc: 0.2396 - val_loss: 2.7366 - val_acc: 0.2384\n",
      "Epoch 10/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6915 - acc: 0.2401 - val_loss: 2.7347 - val_acc: 0.2372\n",
      "Epoch 11/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6848 - acc: 0.2407 - val_loss: 2.7404 - val_acc: 0.2339\n",
      "Epoch 12/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6765 - acc: 0.2416 - val_loss: 2.7377 - val_acc: 0.2370\n",
      "Epoch 13/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6717 - acc: 0.2408 - val_loss: 2.7376 - val_acc: 0.2377\n",
      "Epoch 14/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6634 - acc: 0.2423 - val_loss: 2.7371 - val_acc: 0.2374\n",
      "Epoch 15/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6567 - acc: 0.2418 - val_loss: 2.7381 - val_acc: 0.2357\n",
      "Epoch 16/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6499 - acc: 0.2430 - val_loss: 2.7405 - val_acc: 0.2366\n",
      "Epoch 17/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6417 - acc: 0.2431 - val_loss: 2.7410 - val_acc: 0.2361\n",
      "Epoch 18/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6322 - acc: 0.2442 - val_loss: 2.7439 - val_acc: 0.2375\n",
      "Epoch 19/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6238 - acc: 0.2443 - val_loss: 2.7421 - val_acc: 0.2367\n",
      "Epoch 20/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6157 - acc: 0.2449 - val_loss: 2.7426 - val_acc: 0.2369\n",
      "Epoch 21/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.6058 - acc: 0.2458 - val_loss: 2.7395 - val_acc: 0.2343\n",
      "Epoch 22/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5958 - acc: 0.2473 - val_loss: 2.7448 - val_acc: 0.2365\n",
      "Epoch 23/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5857 - acc: 0.2479 - val_loss: 2.7571 - val_acc: 0.2371\n",
      "Epoch 24/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5771 - acc: 0.2478 - val_loss: 2.7572 - val_acc: 0.2378\n",
      "Epoch 25/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5645 - acc: 0.2502 - val_loss: 2.7544 - val_acc: 0.2360\n",
      "Epoch 26/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5529 - acc: 0.2512 - val_loss: 2.7574 - val_acc: 0.2353\n",
      "Epoch 27/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5401 - acc: 0.2533 - val_loss: 2.7593 - val_acc: 0.2357\n",
      "Epoch 28/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5255 - acc: 0.2540 - val_loss: 2.7642 - val_acc: 0.2348\n",
      "Epoch 29/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5136 - acc: 0.2551 - val_loss: 2.7713 - val_acc: 0.2345\n",
      "Epoch 30/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.5008 - acc: 0.2578 - val_loss: 2.7801 - val_acc: 0.2346\n",
      "Epoch 31/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4866 - acc: 0.2594 - val_loss: 2.7825 - val_acc: 0.2302\n",
      "Epoch 32/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4711 - acc: 0.2613 - val_loss: 2.7926 - val_acc: 0.2294\n",
      "Epoch 33/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4566 - acc: 0.2630 - val_loss: 2.7964 - val_acc: 0.2302\n",
      "Epoch 34/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4393 - acc: 0.2665 - val_loss: 2.8034 - val_acc: 0.2293\n",
      "Epoch 35/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4255 - acc: 0.2673 - val_loss: 2.8137 - val_acc: 0.2308\n",
      "Epoch 36/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.4117 - acc: 0.2712 - val_loss: 2.8247 - val_acc: 0.2286\n",
      "Epoch 37/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3935 - acc: 0.2729 - val_loss: 2.8331 - val_acc: 0.2319\n",
      "Epoch 38/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3747 - acc: 0.2758 - val_loss: 2.8428 - val_acc: 0.2304\n",
      "Epoch 39/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3595 - acc: 0.2790 - val_loss: 2.8492 - val_acc: 0.2224\n",
      "Epoch 40/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3413 - acc: 0.2813 - val_loss: 2.8595 - val_acc: 0.2209\n",
      "Epoch 41/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3278 - acc: 0.2832 - val_loss: 2.8667 - val_acc: 0.2257\n",
      "Epoch 42/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.3087 - acc: 0.2865 - val_loss: 2.8715 - val_acc: 0.2219\n",
      "Epoch 43/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2905 - acc: 0.2890 - val_loss: 2.8751 - val_acc: 0.2241\n",
      "Epoch 44/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2750 - acc: 0.2915 - val_loss: 2.8863 - val_acc: 0.2226\n",
      "Epoch 45/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2590 - acc: 0.2958 - val_loss: 2.8923 - val_acc: 0.2208\n",
      "Epoch 46/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2413 - acc: 0.2983 - val_loss: 2.9024 - val_acc: 0.2214\n",
      "Epoch 47/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2251 - acc: 0.3009 - val_loss: 2.9158 - val_acc: 0.2190\n",
      "Epoch 48/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.2091 - acc: 0.3037 - val_loss: 2.9246 - val_acc: 0.2203\n",
      "Epoch 49/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.1926 - acc: 0.3069 - val_loss: 2.9298 - val_acc: 0.2191\n",
      "Epoch 50/50\n",
      "2483/2483 [==============================] - 64s 26ms/step - loss: 2.1753 - acc: 0.3090 - val_loss: 2.9456 - val_acc: 0.2214\n"
     ]
    }
   ],
   "source": [
    "# Image only model\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.3)\n",
    "config = tf.ConfigProto(gpu_options = gpu_options)\n",
    "config.gpu_options.allow_growth=True\n",
    "session = tf.Session( config = config)\n",
    "\n",
    "import h5py\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Bidirectional, CuDNNLSTM, Flatten, Embedding, concatenate, Conv1D, Input, Embedding\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adadelta\n",
    "import tensorflow.keras.backend as k\n",
    "k.clear_session()\n",
    "\n",
    "dropout_rate = 0.5\n",
    "h5_que = h5py.File('embedding_matrix_tokenize.h5', 'r')\n",
    "embedding_matrix = h5_que['embedding_matrix_tokenize'][:]\n",
    "\n",
    "print (\"Creating image model...\")\n",
    "\n",
    "image_input_model_1 = Input(shape=(4096, ) )\n",
    "dense_img_model_1 = Dense(1024,  activation='relu')(image_input_model_1)\n",
    "\n",
    "dropout_1 = Dropout(0.5)(dense_img_model_1)\n",
    "dense_1 = Dense(1001, activation = \"tanh\")(dropout_1)\n",
    "dropout_2 = Dropout(0.5)(dense_1)    \n",
    "dense_2 = Dense(1001, activation = \"softmax\")(dropout_2)\n",
    "\n",
    "model_9 = Model(inputs=image_input_model_1, outputs=dense_2)\n",
    "opt = SGD(lr = 0.01)\n",
    "model_9.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model_9.summary()\n",
    "\n",
    "def img_generator(answer, img, batch_size, types):\n",
    "    \n",
    "    if types == 'train':\n",
    "        x_img =      HDF5Matrix(img, 'image_train')\n",
    "        y =      HDF5Matrix(answer, 'full_answers_train')\n",
    "        size = x_img.end\n",
    "      \n",
    "    elif types == 'test':\n",
    "        x_img =      HDF5Matrix(img, 'image_val')\n",
    "        y = HDF5Matrix(answer, 'full_answers_val')\n",
    "        size = x_img.end\n",
    "      \n",
    "    idx = 0\n",
    "    while True:\n",
    "      \n",
    "        last_batch = idx + batch_size > size\n",
    "        end = idx + batch_size if not last_batch else size\n",
    "        \n",
    "        yield x_img[idx:end], y[idx:end]\n",
    "        idx = end if not last_batch else 0\n",
    "\n",
    "def data_statistic(train_dataset, test_dataset):\n",
    "    train_x = HDF5Matrix(train_dataset, 'full_answers_train')\n",
    "    test_x = HDF5Matrix(test_dataset, 'full_answers_val')\n",
    "    return train_x.end, test_x.end\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    batch_size = 100\n",
    "    answer_train = 'full_answers_train.h5'\n",
    "    answer_test =  'full_answers_val.h5'\n",
    "    \n",
    "    img_train = 'image_train.h5'\n",
    "    img_test =   'image_val.h5'\n",
    "    \n",
    "    train_generator = img_generator(answer_train, img_train,  batch_size, types = 'train')\n",
    "    test_generator =  img_generator(answer_test,  img_test,   batch_size, types = 'test')\n",
    "    \n",
    "    nb_train_samples, nb_test_samples = data_statistic(answer_train, answer_test)\n",
    "    print('train samples: %d, test samples: %d' % (nb_train_samples, nb_test_samples))\n",
    "\n",
    "    model_9.fit_generator(\n",
    "        epochs=50,\n",
    "        generator=train_generator, steps_per_epoch=nb_train_samples // batch_size,\n",
    "        validation_data=test_generator, validation_steps=nb_test_samples // batch_size,\n",
    "        max_queue_size=10,  # use a value which can fit batch_size * image_size * max_queue_size in your CPU memory\n",
    "        workers=1,  # I don't see multi workers can have any performance benefit without multi threading\n",
    "        use_multiprocessing=False,  # HDF5Matrix cannot support multi-threads\n",
    "        shuffle=False)  # you cannot shuffle on a HDF5Matrix, so make sure you shuffle the data before save to h5 file\n",
    "\n",
    "\n",
    "model_9.save(\"model_9.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IAAvBfaDqHj"
   },
   "source": [
    "Continue to the next notebook for prediction"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xJPMg5Zo47tz",
    "9Yy1JUaDLmq4",
    "ZYm9D9o1Ltml",
    "f_z2iVVy4VRI",
    "tmzPfw2m4ayU",
    "KtL4U_nW4gLI",
    "9Y8tf7jZ4k0E",
    "lgYZuqjX4pLS",
    "-lJajDlG4u9m",
    "UVdIXwIH4yvv",
    "L3TrfGTWeaaq"
   ],
   "name": "Training on Whole dataset.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
